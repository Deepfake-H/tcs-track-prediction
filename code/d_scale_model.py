import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import conv_out_size
import constants as c

class DScaleModel(nn.Module):
    """
    A DScaleModel is a network that takes as input one video frame and attempts to discriminate
    whether the output frame is a real-world image or one generated by a generator network.
    Multiple of these are used together in a DiscriminatorModel to make predictions on frames at
    increasing scales.
    """
    def __init__(self, device, scale_index, height, width, conv_layer_fms, kernel_sizes, fc_layer_sizes):
        """
        Initializes the DScaleModel.
        """
        super(DScaleModel, self).__init__()
        self.device = device
        self.scale_index = scale_index
        self.height = height
        self.width = width
        assert len(kernel_sizes) == len(conv_layer_fms) - 1, \
            'len(kernel_sizes) must = len(conv_layer_fms) - 1'

        # Convolutional layers
        conv_layers = []
        last_channels = conv_layer_fms[0]
        last_out_height = self.height
        last_out_width = self.width
        for out_channels, kernel_size in zip(conv_layer_fms[1:], kernel_sizes):
            conv_layers.append(nn.Conv2d(last_channels, out_channels, kernel_size, padding=c.PADDING_D))
            conv_layers.append(nn.BatchNorm2d(out_channels))
            conv_layers.append(nn.ReLU())
            last_channels = out_channels

            last_out_height = conv_out_size(
                last_out_height, c.PADDING_D, kernel_size, 1)
            last_out_width = conv_out_size(
                last_out_width, c.PADDING_D, kernel_size, 1)
        self.conv_layers = nn.Sequential(*conv_layers)

        # Fully-connected layers
        self.fc_layers = nn.ModuleList()
        # Add in an initial layer to go from the last conv to the first fully-connected.
        # Use /2 for the height and width because there is a 2x2 pooling layer
        input_features = (last_out_height // 2) * (last_out_width // 2) * conv_layer_fms[-1]
        fc_layer_sizes.insert(0, input_features)
        for in_features, out_features in zip(fc_layer_sizes[:-1], fc_layer_sizes[1:]):
            self.fc_layers.append(nn.Linear(in_features, out_features))

    def forward(self, x):
        # Forward pass through conv layers
        for layer in self.conv_layers:
            x = layer(x)

        # Pooling layer
        kernel_size = 2
        padding = 0 if c.PADDING_D == 'valid' else kernel_size // 2
        x = F.max_pool2d(x, kernel_size=kernel_size, stride=2, padding=padding)

        # Flatten
        x = torch.flatten(x, 1)

        # Forward pass through fully-connected layers
        for i, layer in enumerate(self.fc_layers):
            x = layer(x)
            if i < len(self.fc_layers) - 1:
                x = F.relu(x)
            else:
                x = torch.sigmoid(x)

        # Clip predictions for stability
        x = torch.clamp(x, 0.1, 0.9)
        return x